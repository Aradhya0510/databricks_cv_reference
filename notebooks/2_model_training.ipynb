{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda7dc95-fefb-4b5e-ad93-5ce726a369eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TtdHVUigOVxq"
   },
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook demonstrates the model training pipeline using PyTorch Lightning and Ray for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840922ae-3e24-45a5-8ba2-f21c55ef7bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c565b24-1507-4553-9b1b-69500e533ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aliQy1vnOVxr"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install pytorch-lightning ray[default] mlflow pycocotools albumentations\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c46c89e-2c7e-4907-bac3-fdbccff5c9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_K10NGw3OVxs"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import ray\n",
    "from ray import train\n",
    "import pytorch_lightning as pl\n",
    "import mlflow\n",
    "from models.base.base_model import BaseModel, ModelConfig\n",
    "from trainer.ray_trainer import RayTrainer\n",
    "from trainer.base_trainer import BaseTrainer\n",
    "from models.architectures.classification import ClassificationModel\n",
    "from data.processing.coco_processor import COCOProcessor\n",
    "from data.processing.data_loader import COCODataset, get_transforms, create_dataloader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "932b2e20-d824-408d-a6f5-5d28e0d4a343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KJcJZW6fOVxs"
   },
   "source": [
    "## Initialize Ray\n",
    "\n",
    "Set up Ray for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c1fd52-23d7-4ce6-a794-cf83ce62d58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add your project directory to PYTHONPATH\n",
    "project_dir = os.path.abspath(\"..\")  # or the specific path to your project\n",
    "os.environ['PYTHONPATH'] = f\"{project_dir}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "os.getenv(\"PYTHONPATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f999062-4ffa-45f4-b19c-8b07ecf47296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "\n",
    "# Set databricks credentials as env vars\n",
    "mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fdcbe1-75ac-454e-a233-96c2aa427b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MZLIhTUPOVxs"
   },
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init()\n",
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cfd82e-9265-45db-80ae-aa64bd4334fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "# from mlflow.utils.databricks_utils import get_databricks_env_vars\n",
    "# import ray\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# # Cluster cleanup\n",
    "# restart = True\n",
    "# if restart is True:\n",
    "#   try:\n",
    "#     shutdown_ray_cluster()\n",
    "#   except:\n",
    "#     pass\n",
    "\n",
    "#   try:\n",
    "#     ray.shutdown()\n",
    "#   except:\n",
    "#     pass\n",
    "\n",
    "# # Set configs based on your cluster size\n",
    "# num_cpu_cores_per_worker = 47\n",
    "# num_cpus_head_node = 12\n",
    "# num_gpus_head_node = 1\n",
    "# num_gpus_worker_node = 4\n",
    "\n",
    "# # Set databricks credentials as env vars\n",
    "# mlflow_dbrx_creds = get_databricks_env_vars(\"databricks\")\n",
    "# os.environ[\"DATABRICKS_HOST\"] = mlflow_dbrx_creds['DATABRICKS_HOST']\n",
    "# os.environ[\"DATABRICKS_TOKEN\"] = mlflow_dbrx_creds['DATABRICKS_TOKEN']\n",
    "\n",
    "# # Get the current working directory to include in the Python path\n",
    "# current_dir = os.getcwd()\n",
    "# workspace_dir = \"/Workspace/Users/alex.miller@databricks.com/databricks-cv-architecture\"\n",
    "\n",
    "# # Configure runtime environment to include your custom modules\n",
    "# runtime_env = {\n",
    "#     \"py_modules\": [workspace_dir],  # Include your workspace directory\n",
    "#     \"env_vars\": {\n",
    "#         \"PYTHONPATH\": f\"{workspace_dir}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# ray_conf = setup_ray_cluster(\n",
    "#   min_worker_nodes=1,\n",
    "#   max_worker_nodes=1,\n",
    "#   num_cpus_head_node=num_cpus_head_node,\n",
    "#   num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "#   num_gpus_head_node=num_gpus_head_node,\n",
    "#   num_gpus_worker_node=num_gpus_worker_node,\n",
    "#   runtime_env=runtime_env  # Pass the runtime environment\n",
    "# )\n",
    "\n",
    "# os.environ['RAY_ADDRESS'] = ray_conf[0]\n",
    "# ray.init(runtime_env=runtime_env)  # Also set runtime_env here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d78ebcc-acb6-4f60-be8b-cef1a461094e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BgI4x1b3OVxs"
   },
   "source": [
    "## Configure Model\n",
    "\n",
    "Set up model configuration and architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835f7d9b-292b-4d7f-ac04-fc3f20c1969c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DOA14thaOVxs"
   },
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "config = ModelConfig(\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    task='classification',\n",
    "    optimizer='adamw',\n",
    "    scheduler='cosine',\n",
    "    scheduler_params={'T_max': 15, 'eta_min': 1e-6},  # Explicitly set T_max\n",
    "    max_epochs=1\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "classification_model = ClassificationModel()\n",
    "model = BaseModel(model=classification_model, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "620854ea-5b04-49c1-9ea3-9b0e0ee959c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Dataloader:\n",
    "- Option 1: COCODataset -> PyTorch dataloader -> pass into Ray trainer\n",
    "- Option 2: Delta table -> Ray data -> pass into Ray trainer\n",
    "- Option 3: COCODataset -> Ray data -> pass into Ray trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8408e49-6f8a-46fc-8447-bcfdd9be3eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "image_dir = \"/Volumes/users/aradhya_chouhan/coco_mini-train/data/val2017/\"\n",
    "annotation_file = \"/Volumes/users/aradhya_chouhan/coco_mini-train/data/instances_val2017.json\"\n",
    "\n",
    "dataset = COCODataset(image_dir=image_dir, annotation_file=annotation_file, transform=get_transforms(mode='train'))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "# Create dataloader\n",
    "train_dataloader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = create_dataloader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1a1913d-3900-473d-beda-41664d682cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fJUigTyYOVxt"
   },
   "source": [
    "## Set Up Training\n",
    "\n",
    "Configure the training pipeline with Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c7b9b0-50e5-45e7-b6d2-2a8576a520b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tSvX3eDBOVxt"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "base_trainer = BaseTrainer(\n",
    "    model=model,\n",
    "    model_config=config\n",
    ")\n",
    "trainer = RayTrainer(\n",
    "    trainer=base_trainer,\n",
    "    num_workers=int(ray.cluster_resources().get(\"GPU\")),\n",
    "    use_gpu=True,\n",
    "    databricks_host=os.environ[\"DATABRICKS_HOST\"],\n",
    "    databricks_token=os.environ[\"DATABRICKS_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Configure training\n",
    "# notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "training_config = {\n",
    "    \"experiment_name\": f\"/Users/alex.miller@databricks.com/cv_experiment\",\n",
    "    \"run_name\": \"training_run_1\",\n",
    "    \"max_epochs\": 100,\n",
    "    \"checkpoint_dir\": \"/Volumes/main/alex_m/coco_dataset/checkpoints\",\n",
    "    \"model_path\": \"/Volumes/main/alex_m/coco_dataset/model\",\n",
    "    \"train_loader\": train_dataloader,\n",
    "    \"val_loader\": val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c26f48-0a3a-4e2e-b932-8b21382e235b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8056e7-c834-4c3e-9e6d-03f2b3407051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.mkdir(\"/Volumes/main/alex_m/coco_dataset/checkpoints\")\n",
    "# os.mkdir(\"/Volumes/main/alex_m/coco_dataset/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3112638-33aa-4502-b438-ca7d2fb08895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jo5ASSSdOVxt"
   },
   "source": [
    "## Start Training\n",
    "\n",
    "Begin the distributed training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84fa90d2-297f-47a0-afce-c4c58281d697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vc3VslZVOVxt"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "result = trainer.train(training_config)\n",
    "\n",
    "# Display training results\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best model path: {result['best_model_path']}\")\n",
    "print(f\"Final metrics: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "687264a3-5956-4aea-b63b-9b1e08e37bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6QgkT5u8OVxt"
   },
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Plot training metrics and learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a63eb38-ad8a-4996-9437-bd725d17df3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rjI7RON-OVxt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
    "    plt.plot(metrics['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['learning_rate'], label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training metrics\n",
    "plot_metrics(result['metrics'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2_model_training",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
